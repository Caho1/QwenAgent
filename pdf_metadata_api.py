# -*- coding: utf-8 -*-
"""
PDFå…ƒæ•°æ®æå–ç³»ç»Ÿ - Flask APIåç«¯
è½»é‡åŒ–è®¾è®¡ï¼Œé›†æˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—
"""

import os
import json
import uuid
import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import asdict
from pathlib import Path

from flask import Flask, request, jsonify, Response, render_template, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd

# å¯¼å…¥ç°æœ‰çš„å…ƒæ•°æ®æå–æ¨¡å—
from Metadata import extract_first_page_llm, PaperMeta, extract_acknowledgment_from_last_pages
from concurrent_processor import get_global_processor, ConcurrentProcessor, RateLimitConfig
from config import Config
from log_manager import log_manager, log_operation, log_file_upload, log_file_processing, log_batch_processing, log_api_call

# =========================
# é…ç½®åˆå§‹åŒ–
# =========================

# =========================
# æ•°æ®å¤„ç†å™¨ç±»
# =========================
class MetadataProcessor:
    """å…ƒæ•°æ®å¤„ç†å™¨ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘"""
    
    def __init__(self):
        self.processing_tasks = {}
    
    def _extract_real_filename(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–çœŸå®çš„æ–‡ä»¶åï¼ˆå»æ‰UUIDå‰ç¼€ï¼‰ï¼Œä¿ç•™æ‰©å±•å"""
        filename = os.path.basename(file_path)
        if '_' in filename:
            # æ£€æŸ¥ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¦æ˜¯UUIDæ ¼å¼ï¼ˆ8-4-4-4-12ä¸ªå­—ç¬¦ï¼‰
            parts = filename.split('_', 1)
            if len(parts) == 2:
                potential_uuid = parts[0]
                # ç®€å•çš„UUIDæ ¼å¼æ£€æŸ¥ï¼šé•¿åº¦ä¸º36ä¸”åŒ…å«4ä¸ªè¿å­—ç¬¦
                if len(potential_uuid) == 36 and potential_uuid.count('-') == 4:
                    # æ ¼å¼ï¼šUUID_çœŸå®æ–‡ä»¶å.pdf
                    return parts[1]

        # å¦‚æœæ²¡æœ‰UUIDå‰ç¼€ï¼Œç›´æ¥è¿”å›æ–‡ä»¶å
        return filename

    def _clean_export_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¸…ç†å¯¼å‡ºæ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ"""
        # éœ€è¦ç§»é™¤çš„å†…éƒ¨å­—æ®µ
        internal_fields = {
            '_original_index',
            '_upload_order',
            'attempt',
            'processing_time',
            'filename',  # ç§»é™¤é€šç”¨filenameå­—æ®µ
            'file',      # ç§»é™¤æ–‡ä»¶è·¯å¾„å­—æ®µ
            'status'     # ç§»é™¤çŠ¶æ€å­—æ®µï¼ˆä»…ä¿ç•™æœ‰é”™è¯¯çš„è®°å½•ä¸­çš„errorå­—æ®µï¼‰
        }

        cleaned_data = []
        for item in data:
            # è·³è¿‡æœ‰é”™è¯¯çš„è®°å½•
            if item.get('error') or item.get('status') == 'failed':
                continue

            # åˆ›å»ºæ¸…ç†åçš„è®°å½•
            cleaned_item = {}
            for key, value in item.items():
                if key not in internal_fields:
                    cleaned_item[key] = value

            cleaned_data.append(cleaned_item)

        return cleaned_data

    async def process_file(self, file_path: str, mode: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        try:
            # è°ƒç”¨ç°æœ‰çš„å…ƒæ•°æ®æå–å‡½æ•°
            meta = await extract_first_page_llm(file_path)
            
            # æ ¹æ®æ¨¡å¼è½¬æ¢æ•°æ®æ ¼å¼
            if mode == 'sn':
                return self._format_sn_data(meta, file_path)
            elif mode == 'ieee':
                return self._format_ieee_data(meta, file_path)
            elif mode == 'funding':
                return self._format_funding_data(meta, file_path)
            elif mode == 'ap':
                return self._format_ap_data(meta, file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
                
        except Exception as e:
            filename = self._extract_real_filename(file_path)

            # ä¸ºIEEEæ¨¡å¼çš„è®¢å•å·å»é™¤.pdfæ‰©å±•å
            order_number = filename
            if mode == 'ieee' and order_number.lower().endswith('.pdf'):
                order_number = order_number[:-4]

            return {
                'error': str(e),
                'file': file_path,
                'filename': filename,
                # æ ¹æ®æ¨¡å¼æ·»åŠ å¯¹åº”çš„æ–‡ä»¶åå­—æ®µ
                'æ–‡ä»¶å': filename,  # èµ„åŠ©ä¿¡æ¯å’ŒAPæ¨¡å¼
                'Number': filename,  # SNæ¨¡å¼
                'è®¢å•å·': order_number,  # IEEEæ¨¡å¼ï¼ˆå»é™¤.pdfæ‰©å±•åï¼‰
                'status': 'failed'
            }
    
    def _format_sn_data(self, meta: PaperMeta, file_path: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–SNæ¨¡å¼æ•°æ®"""
        filename = self._extract_real_filename(file_path)
        result = {
            'Number': filename,
            'Title': meta.title,
            'SubTitle': '',  # éœ€è¦ä»æ ‡é¢˜ä¸­åˆ†ç¦»å‰¯æ ‡é¢˜
            'Corresponding Author': '',
            "Corresponding author's email": '',
            'filename': filename  # æ·»åŠ é€šç”¨filenameå­—æ®µ
        }
        
        # å¤„ç†ä½œè€…ä¿¡æ¯
        for i, author in enumerate(meta.authors[:5], 1):
            result[f'Author {i}'] = author.name
            result[f'Affiliation {i}'] = next(
                (aff.name for aff in meta.affiliations if aff.id in author.affiliation_ids),
                ''
            )
            
            # è¯†åˆ«é€šè®¯ä½œè€…
            if author.is_corresponding_author:
                result['Corresponding Author'] = author.name
                result["Corresponding author's email"] = author.email or ''
        
        # å¦‚æœæ²¡æœ‰æ ‡è®°é€šè®¯ä½œè€…ï¼Œä½¿ç”¨ç¬¬ä¸€ä½œè€…
        if not result['Corresponding Author'] and meta.authors:
            result['Corresponding Author'] = meta.authors[0].name
            result["Corresponding author's email"] = meta.authors[0].email or ''
        
        return result
    
    def _format_ieee_data(self, meta: PaperMeta, file_path: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–IEEEæ¨¡å¼æ•°æ®"""
        # æå–æ‰€æœ‰ä½œè€…å§“åï¼Œå»é™¤ä¸Šæ ‡
        all_authors = ', '.join([author.name for author in meta.authors])

        # è·å–ç¬¬ä¸€ä½œè€…é‚®ç®±ï¼Œå¦‚æœæ²¡æœ‰åˆ™å–é€šè®¯ä½œè€…é‚®ç®±
        first_author_email = ''
        if meta.authors:
            first_author_email = meta.authors[0].email or ''
            if not first_author_email:
                # æŸ¥æ‰¾é€šè®¯ä½œè€…é‚®ç®±
                for author in meta.authors:
                    if author.is_corresponding_author and author.email:
                        first_author_email = author.email
                        break

        filename = self._extract_real_filename(file_path)
        # å¯¹äºè®¢å•å·å­—æ®µï¼Œå»é™¤.pdfæ‰©å±•å
        order_number = filename
        if order_number.lower().endswith('.pdf'):
            order_number = order_number[:-4]

        # æŒ‰ç…§æŒ‡å®šé¡ºåºè¿”å›å­—æ®µ
        return {
            'è®¢å•å·': order_number,
            'è‹±æ–‡é¢˜ç›®': meta.title,
            'è‹±æ–‡å‰¯æ ‡': '',  # éœ€è¦ä»æ ‡é¢˜ä¸­åˆ†ç¦»
            'ä½œè€…å§“å': all_authors,
            'ç¬¬ä¸€ä½œè€…é‚®ç®±': first_author_email,
            'filename': filename  # æ·»åŠ é€šç”¨filenameå­—æ®µï¼ˆç”¨äºå†…éƒ¨å¤„ç†ï¼‰
        }
    
    def _format_funding_data(self, meta: PaperMeta, file_path: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–èµ„åŠ©ä¿¡æ¯æ¨¡å¼æ•°æ®"""
        first_author = meta.authors[0] if meta.authors else None
        corresponding_author = next(
            (author for author in meta.authors if author.is_corresponding_author),
            first_author
        )

        # æå–è‡´è°¢ä¿¡æ¯
        acknowledgment = ""
        try:
            acknowledgment = extract_acknowledgment_from_last_pages(file_path)
        except Exception as e:
            print(f"è‡´è°¢ä¿¡æ¯æå–å¤±è´¥: {e}")

        filename = self._extract_real_filename(file_path)
        return {
            'æ–‡ä»¶å': filename,
            'è®ºæ–‡è‹±æ–‡é¢˜ç›®': meta.title,
            'ç¬¬ä¸€ä½œè€…å§“å': first_author.name if first_author else '',
            'ç¬¬ä¸€ä½œè€…å•ä½': self._get_author_affiliation(first_author, meta.affiliations) if first_author else '',
            'é€šè®¯ä½œè€…å§“å': corresponding_author.name if corresponding_author else '',
            'é€šè®¯ä½œè€…å•ä½': self._get_author_affiliation(corresponding_author, meta.affiliations) if corresponding_author else '',
            'é€šè®¯ä½œè€…é‚®ç®±': corresponding_author.email if corresponding_author else '',
            'å…³é”®è¯': ', '.join(meta.keywords),
            'æ‘˜è¦': meta.abstract or '',
            'è‡´è°¢': acknowledgment,
            'filename': filename  # æ·»åŠ é€šç”¨filenameå­—æ®µ
        }
    
    def _format_ap_data(self, meta: PaperMeta, file_path: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–APæ¨¡å¼æ•°æ®"""
        first_author = meta.authors[0] if meta.authors else None
        corresponding_author = next(
            (author for author in meta.authors if author.is_corresponding_author),
            None
        )
        
        filename = self._extract_real_filename(file_path)
        result = {
            'é¢˜ç›®': meta.title,
            'å…³é”®è¯': ', '.join(meta.keywords),
            'æ‘˜è¦': meta.abstract or '',
            'æ–‡ä»¶å': filename,
            'filename': filename  # æ·»åŠ é€šç”¨filenameå­—æ®µ
        }
        
        # ç¬¬ä¸€ä½œè€…å§“ååˆ†è§£
        if first_author:
            name_parts = first_author.name.split()
            result['ç¬¬ä¸€ä½œè€…å§“'] = name_parts[-1] if name_parts else ''
            result['ç¬¬ä¸€ä½œè€…å'] = ' '.join(name_parts[:-1]) if len(name_parts) > 1 else ''
        
        # é€šè®¯ä½œè€…å§“ååˆ†è§£ï¼ˆä»…å½“é€šè®¯ä½œè€…éç¬¬ä¸€ä½œè€…æ—¶ï¼‰
        if corresponding_author and corresponding_author != first_author:
            name_parts = corresponding_author.name.split()
            result['é€šè®¯ä½œè€…å§“'] = name_parts[-1] if name_parts else ''
            result['é€šè®¯ä½œè€…å'] = ' '.join(name_parts[:-1]) if len(name_parts) > 1 else ''
        
        return result
    
    def _get_author_affiliation(self, author, affiliations) -> str:
        """è·å–ä½œè€…å•ä½"""
        if not author or not author.affiliation_ids:
            return ''
        
        for aff_id in author.affiliation_ids:
            aff = next((aff for aff in affiliations if aff.id == aff_id), None)
            if aff:
                return aff.name
        return ''

# =========================
# Flaskåº”ç”¨åˆå§‹åŒ–
# =========================
app = Flask(__name__)
app.config.from_object(Config)
CORS(app)

# åˆå§‹åŒ–ç»„ä»¶
Config.init_app(app)
processor = MetadataProcessor()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_extraction.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

# =========================
# APIè·¯ç”±å®šä¹‰
# =========================

@app.route('/')
def index():
    """ä¸»é¡µ - è¿”å›å‰ç«¯ç•Œé¢"""
    return render_template('PDF.html')

@app.route('/favicon.ico')
def favicon():
    """Favicon"""
    return '', 204

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

@app.route('/api/upload', methods=['POST'])
def upload_files():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    start_time = time.time()
    try:
        files = request.files.getlist('files')
        if not files or all(not file for file in files):
            log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}, time.time() - start_time, "error")
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400

        uploaded_files = []
        errors = []

        for file in files:
            if not file:
                continue

            if not file.filename:
                errors.append({'filename': 'æœªçŸ¥', 'error': 'æ–‡ä»¶åä¸ºç©º'})
                continue

            if not allowed_file(file.filename):
                errors.append({'filename': file.filename, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒPDFæ–‡ä»¶'})
                continue

            try:
                filename = secure_filename(file.filename)
                file_id = str(uuid.uuid4())
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
                file.save(file_path)

                # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                if not os.path.exists(file_path):
                    errors.append({'filename': filename, 'error': 'æ–‡ä»¶ä¿å­˜å¤±è´¥'})
                    continue

                file_size = os.path.getsize(file_path)
                if file_size == 0:
                    errors.append({'filename': filename, 'error': 'æ–‡ä»¶ä¸ºç©º'})
                    os.remove(file_path)  # åˆ é™¤ç©ºæ–‡ä»¶
                    continue

                # è®°å½•æ–‡ä»¶ä¸Šä¼ æ—¥å¿—
                log_file_upload(filename, file_size)

                uploaded_files.append({
                    'file_id': file_id,
                    'filename': filename,
                    'path': file_path,
                    'size': file_size
                })

            except Exception as e:
                errors.append({'filename': file.filename, 'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'})

        processing_time = time.time() - start_time
        
        if not uploaded_files and errors:
            log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": "æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥", "errors": errors}, processing_time, "error")
            return jsonify({
                'success': False,
                'error': 'æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥',
                'errors': errors
            }), 400

        # è®°å½•æ‰¹é‡ä¸Šä¼ æˆåŠŸæ—¥å¿—
        log_batch_processing(len(uploaded_files), "æ–‡ä»¶ä¸Šä¼ ", processing_time, len(uploaded_files), len(errors))
        
        return jsonify({
            'success': True,
            'files': uploaded_files,
            'count': len(uploaded_files),
            'errors': errors if errors else None
        })

    except Exception as e:
        processing_time = time.time() - start_time
        log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}, processing_time, "error")
        logging.error(f"æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/files', methods=['GET'])
def list_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    try:
        files = []
        upload_dir = Path(app.config['UPLOAD_FOLDER'])

        for file_path in upload_dir.glob('*.pdf'):
            stat = file_path.stat()
            # ä»æ–‡ä»¶åä¸­æå–çœŸå®æ–‡ä»¶åï¼ˆå»æ‰UUIDå‰ç¼€ï¼‰
            full_filename = file_path.name
            if '_' in full_filename:
                # æ ¼å¼ï¼šUUID_çœŸå®æ–‡ä»¶å.pdf
                real_filename = '_'.join(full_filename.split('_')[1:])
            else:
                real_filename = full_filename
            
            files.append({
                'filename': real_filename,  # è¿”å›çœŸå®æ–‡ä»¶å
                'full_filename': full_filename,  # ä¿ç•™å®Œæ•´æ–‡ä»¶åç”¨äºåˆ é™¤ç­‰æ“ä½œ
                'size': stat.st_size,
                'upload_time': datetime.fromtimestamp(stat.st_ctime).isoformat()
            })

        return jsonify({
            'files': files,
            'count': len(files)
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/files/<file_id>', methods=['DELETE'])
def delete_file(file_id):
    """åˆ é™¤æŒ‡å®šæ–‡ä»¶"""
    try:
        upload_dir = Path(app.config['UPLOAD_FOLDER'])
        file_pattern = f"{file_id}_*"

        deleted = False
        for file_path in upload_dir.glob(file_pattern):
            file_path.unlink()
            deleted = True

        if deleted:
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/<mode>', methods=['POST'])
def extract_metadata(mode):
    """å•æ¨¡å¼å…ƒæ•°æ®æå–æ¥å£ï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰"""
    try:
        if mode not in ['sn', 'ieee', 'funding', 'ap']:
            return jsonify({'error': f'ä¸æ”¯æŒçš„æ¨¡å¼: {mode}'}), 400

        data = request.get_json()
        file_paths = data.get('file_paths', [])
        use_concurrent = data.get('use_concurrent', len(file_paths) > 5)  # è¶…è¿‡5ä¸ªæ–‡ä»¶è‡ªåŠ¨å¯ç”¨å¹¶å‘

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_files = []
        invalid_files = []
        for file_path in file_paths:
            if os.path.exists(file_path):
                valid_files.append(file_path)
            else:
                invalid_files.append({
                    'file': file_path,
                    'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
                    'status': 'failed'
                })

        if not valid_files:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„',
                'results': invalid_files
            }), 400

        # é€‰æ‹©å¤„ç†æ–¹å¼
        if use_concurrent and len(valid_files) > 1:
            # å¹¶å‘å¤„ç†
            concurrent_processor = get_global_processor()

            async def process_wrapper(file_path: str, mode: str):
                return await processor.process_file(file_path, mode)

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                results = loop.run_until_complete(
                    concurrent_processor.process_batch(valid_files, process_wrapper, mode)
                )
                # å¹¶å‘å¤„ç†å™¨å†…éƒ¨å·²ç»æŒ‰åŸå§‹ç´¢å¼•æ’åºï¼Œæ— éœ€é¢å¤–æ’åº
            finally:
                loop.close()
        else:
            # ä¸²è¡Œå¤„ç†
            results = []
            for file_path in valid_files:
                try:
                    # åœ¨Flaskçº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                    import concurrent.futures

                    def run_async_in_thread():
                        """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            return loop.run_until_complete(processor.process_file(file_path, mode))
                        finally:
                            loop.close()
                    
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥å‡½æ•°
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    
                    results.append(result)
                except Exception as e:
                    logger.error(f"ä¸²è¡Œå¤„ç†æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
                    results.append({
                        'file': file_path,
                        'filename': processor._extract_real_filename(file_path),
                        'error': str(e),
                        'status': 'failed'
                    })

        # åˆå¹¶æ— æ•ˆæ–‡ä»¶ç»“æœï¼Œå¹¶æŒ‰ç…§åŸå§‹æ–‡ä»¶è·¯å¾„é¡ºåºæ’åº
        all_results = results + invalid_files
        
        # åˆ›å»ºä¸€ä¸ªæ˜ å°„æ¥ä¿æŒåŸå§‹é¡ºåº
        file_order_map = {file_path: i for i, file_path in enumerate(file_paths)}
        
        # æŒ‰ç…§åŸå§‹æ–‡ä»¶è·¯å¾„é¡ºåºæ’åºæ‰€æœ‰ç»“æœ
        def get_original_order(result):
            file_path = result.get('file', '')
            return file_order_map.get(file_path, 999999)  # æ— æ•ˆæ–‡ä»¶æ’åœ¨æœ€å
        
        all_results.sort(key=get_original_order)
        
        successful_count = len([r for r in results if r.get('status') != 'failed' and 'error' not in r])

        return jsonify({
            'success': True,
            'mode': mode,
            'results': all_results,
            'count': len(all_results),
            'successful': successful_count,
            'failed': len(all_results) - successful_count,
            'concurrent_used': use_concurrent and len(valid_files) > 1
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/batch', methods=['POST'])
def extract_batch():
    """ä¸“é—¨çš„æ‰¹é‡å¤„ç†ç«¯ç‚¹ï¼ˆä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†ï¼‰"""
    try:
        data = request.get_json()
        file_paths = data.get('file_paths', [])
        mode = data.get('mode', 'sn')

        if mode not in ['sn', 'ieee', 'funding', 'ap']:
            return jsonify({'error': f'ä¸æ”¯æŒçš„æ¨¡å¼: {mode}'}), 400

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        if len(file_paths) > 200:
            return jsonify({'error': 'å•æ¬¡æœ€å¤šå¤„ç†200ä¸ªæ–‡ä»¶'}), 400

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_files = [f for f in file_paths if os.path.exists(f)]
        invalid_count = len(file_paths) - len(valid_files)

        if not valid_files:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„'}), 400

        # ä½¿ç”¨å¹¶å‘å¤„ç†å™¨
        concurrent_processor = get_global_processor()

        async def process_wrapper(file_path: str, mode: str):
            return await processor.process_file(file_path, mode)

        # è¿›åº¦è·Ÿè¸ª
        progress_info = {'current': 0, 'total': len(valid_files), 'message': 'å‡†å¤‡ä¸­...'}

        async def progress_callback(progress: float, message: str):
            progress_info['current'] = int(progress * len(valid_files) / 100)
            progress_info['message'] = message

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            start_time = time.time()
            results = loop.run_until_complete(
                concurrent_processor.process_batch(valid_files, process_wrapper, mode, progress_callback)
            )
            processing_time = time.time() - start_time

            # å¹¶å‘å¤„ç†å™¨å†…éƒ¨å·²ç»æŒ‰åŸå§‹ç´¢å¼•æ’åºï¼Œæ— éœ€é¢å¤–æ’åº
            
        finally:
            loop.close()

        # ç»Ÿè®¡ç»“æœ
        successful_count = len([r for r in results if r.get('status') != 'failed' and 'error' not in r])
        failed_count = len(results) - successful_count

        # è·å–å¤„ç†ç»Ÿè®¡
        stats = concurrent_processor.get_processing_stats()

        return jsonify({
            'success': True,
            'mode': mode,
            'results': results,
            'statistics': {
                'total_files': len(file_paths),
                'valid_files': len(valid_files),
                'invalid_files': invalid_count,
                'successful': successful_count,
                'failed': failed_count,
                'success_rate': (successful_count / len(valid_files)) * 100 if valid_files else 0,
                'processing_time': round(processing_time, 2),
                'avg_time_per_file': round(processing_time / len(valid_files), 2) if valid_files else 0
            },
            'rate_limit_stats': stats
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/processing/stats', methods=['GET'])
def get_processing_stats():
    """è·å–å½“å‰å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        concurrent_processor = get_global_processor()
        stats = concurrent_processor.get_processing_stats()

        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': time.time()
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/batch', methods=['POST'])
def batch_extract():
    """æ‰¹é‡å¤„ç†å¤šç§æ¨¡å¼"""
    try:
        data = request.get_json()
        file_paths = data.get('file_paths', [])
        modes = data.get('modes', ['sn'])

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        results = {}
        for mode in modes:
            if mode not in ['sn', 'ieee', 'funding', 'ap']:
                continue

            mode_results = []
            for file_path in file_paths:
                if not os.path.exists(file_path):
                    mode_results.append({
                        'file': file_path,
                        'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
                        'status': 'failed'
                    })
                    continue

                # å¼‚æ­¥å¤„ç†æ–‡ä»¶
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(processor.process_file(file_path, mode))
                    mode_results.append(result)
                finally:
                    loop.close()

            results[mode] = mode_results

        return jsonify({
            'success': True,
            'results': results,
            'processed_modes': list(results.keys())
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/process', methods=['POST'])
def process_files():
    """å¤„ç†PDFæ–‡ä»¶çš„ä¸»æ¥å£ï¼ˆæµå¼å“åº”ï¼‰"""
    start_time = time.time()
    
    def generate():
        try:
            # è·å–ä¸Šä¼ çš„æ–‡ä»¶å’Œæ¨¡å¼
            files = request.files.getlist('files')
            mode = request.form.get('mode', 'sn')

            if not files:
                log_operation("æ–‡ä»¶å¤„ç†", {"error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}, time.time() - start_time, "error")
                yield json.dumps({'type': 'error', 'message': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}) + '\n'
                return

            # è®°å½•å¼€å§‹å¤„ç†æ—¥å¿—
            log_operation("æ–‡ä»¶å¤„ç†", {"file_count": len(files), "mode": mode})

            yield json.dumps({
                'type': 'status',
                'message': f'å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼Œæ¨¡å¼: {mode}'
            }) + '\n'

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            saved_files = []
            for file in files:
                if file and allowed_file(file.filename):
                    filename = secure_filename(file.filename)
                    file_id = str(uuid.uuid4())
                    file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
                    file.save(file_path)
                    saved_files.append(file_path)

            yield json.dumps({
                'type': 'info',
                'message': f'æˆåŠŸä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶'
            }) + '\n'

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            success_count = 0
            failed_count = 0
            
            for i, file_path in enumerate(saved_files, 1):
                file_start_time = time.time()
                # æå–çœŸå®æ–‡ä»¶åï¼ˆå»æ‰UUIDå‰ç¼€ï¼‰
                real_filename = processor._extract_real_filename(file_path)
                
                yield json.dumps({
                    'type': 'status',
                    'message': f'æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(saved_files)} ä¸ªæ–‡ä»¶: {real_filename}'
                }) + '\n'

                # å¼‚æ­¥å¤„ç†æ–‡ä»¶
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(processor.process_file(file_path, mode))
                    file_processing_time = time.time() - file_start_time

                    if 'error' not in result:
                        success_count += 1
                        # è®°å½•æˆåŠŸå¤„ç†æ—¥å¿—
                        log_file_processing(real_filename, mode, file_processing_time, "success")
                        
                        yield json.dumps({
                            'type': 'data_row',
                            'data': result
                        }) + '\n'

                        yield json.dumps({
                            'type': 'success',
                            'message': f'âœ… {real_filename} å¤„ç†å®Œæˆ'
                        }) + '\n'
                    else:
                        failed_count += 1
                        # è®°å½•å¤±è´¥å¤„ç†æ—¥å¿—
                        log_file_processing(real_filename, mode, file_processing_time, "error", result["error"])
                        
                        yield json.dumps({
                            'type': 'error',
                            'message': f'âŒ {real_filename} å¤„ç†å¤±è´¥: {result["error"]}'
                        }) + '\n'

                finally:
                    loop.close()

            # è®°å½•æ‰¹é‡å¤„ç†å®Œæˆæ—¥å¿—
            total_time = time.time() - start_time
            log_batch_processing(len(saved_files), mode, total_time, success_count, failed_count)
            
            yield json.dumps({
                'type': 'status',
                'message': 'æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ'
            }) + '\n'

        except Exception as e:
            processing_time = time.time() - start_time
            log_operation("æ–‡ä»¶å¤„ç†", {"error": str(e)}, processing_time, "error")
            yield json.dumps({
                'type': 'error',
                'message': f'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
            }) + '\n'

    return Response(generate(), mimetype='text/plain')

@app.route('/api/export/excel', methods=['POST'])
def export_excel():
    """å¯¼å‡ºExcelæ ¼å¼ç»“æœ"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯å¯¼å‡º'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å‡º'}), 400

        # æ ¹æ®æ¨¡å¼å®šä¹‰å­—æ®µé¡ºåº
        column_orders = {
            'ieee': ['è®¢å•å·', 'è‹±æ–‡é¢˜ç›®', 'è‹±æ–‡å‰¯æ ‡', 'ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…é‚®ç®±'],
            'sn': ['Number', 'Title', 'SubTitle', 'Author 1', 'Affiliation 1', 'Author 2', 'Affiliation 2',
                   'Author 3', 'Affiliation 3', 'Author 4', 'Affiliation 4', 'Author 5', 'Affiliation 5',
                   'Corresponding Author', "Corresponding author's email"],
            'funding': ['æ–‡ä»¶å', 'è®ºæ–‡è‹±æ–‡é¢˜ç›®', 'ç¬¬ä¸€ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…å•ä½', 'é€šè®¯ä½œè€…å§“å', 'é€šè®¯ä½œè€…å•ä½',
                       'é€šè®¯ä½œè€…é‚®ç®±', 'å…³é”®è¯', 'æ‘˜è¦', 'è‡´è°¢'],
            'ap': ['æ–‡ä»¶å', 'é¢˜ç›®', 'å…³é”®è¯', 'æ‘˜è¦', 'ç¬¬ä¸€ä½œè€…å§“', 'ç¬¬ä¸€ä½œè€…å', 'é€šè®¯ä½œè€…å§“', 'é€šè®¯ä½œè€…å']
        }

        # è·å–å½“å‰æ¨¡å¼çš„å­—æ®µé¡ºåº
        if mode in column_orders:
            # æŒ‰æŒ‡å®šé¡ºåºé‡æ–°ç»„ç»‡æ•°æ®
            ordered_results = []
            for item in cleaned_results:
                ordered_item = {}
                # å…ˆæŒ‰æŒ‡å®šé¡ºåºæ·»åŠ å­—æ®µ
                for col in column_orders[mode]:
                    if col in item:
                        ordered_item[col] = item[col]
                # å†æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                for key, value in item.items():
                    if key not in ordered_item:
                        ordered_item[key] = value
                ordered_results.append(ordered_item)
            cleaned_results = ordered_results

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(cleaned_results)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{mode}_metadata_{timestamp}.xlsx"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜Excelæ–‡ä»¶
        df.to_excel(file_path, index=False, engine='openpyxl')

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/export/json', methods=['POST'])
def export_json():
    """å¯¼å‡ºJSONæ ¼å¼ç»“æœ"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯å¯¼å‡º'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å‡º'}), 400

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{mode}_metadata_{timestamp}.json"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜JSONæ–‡ä»¶
        export_data = {
            'mode': mode,
            'export_time': datetime.now().isoformat(),
            'count': len(cleaned_results),
            'results': cleaned_results
        }

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/json'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/download/excel', methods=['POST'])
def download_excel():
    """ç›´æ¥ä¸‹è½½Excelæ–‡ä»¶æ¥å£"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯ä¸‹è½½'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¸‹è½½'}), 400

        # æ ¹æ®æ¨¡å¼å®šä¹‰å­—æ®µé¡ºåº
        column_orders = {
            'ieee': ['è®¢å•å·', 'è‹±æ–‡é¢˜ç›®', 'è‹±æ–‡å‰¯æ ‡', 'ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…é‚®ç®±'],
            'sn': ['Number', 'Title', 'SubTitle', 'Author 1', 'Affiliation 1', 'Author 2', 'Affiliation 2',
                   'Author 3', 'Affiliation 3', 'Author 4', 'Affiliation 4', 'Author 5', 'Affiliation 5',
                   'Corresponding Author', "Corresponding author's email"],
            'funding': ['æ–‡ä»¶å', 'è®ºæ–‡è‹±æ–‡é¢˜ç›®', 'ç¬¬ä¸€ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…å•ä½', 'é€šè®¯ä½œè€…å§“å', 'é€šè®¯ä½œè€…å•ä½',
                       'é€šè®¯ä½œè€…é‚®ç®±', 'å…³é”®è¯', 'æ‘˜è¦', 'è‡´è°¢'],
            'ap': ['æ–‡ä»¶å', 'é¢˜ç›®', 'å…³é”®è¯', 'æ‘˜è¦', 'ç¬¬ä¸€ä½œè€…å§“', 'ç¬¬ä¸€ä½œè€…å', 'é€šè®¯ä½œè€…å§“', 'é€šè®¯ä½œè€…å']
        }

        # æŒ‰æŒ‡å®šé¡ºåºé‡æ–°ç»„ç»‡æ•°æ®
        if mode in column_orders:
            ordered_results = []
            for item in cleaned_results:
                ordered_item = {}
                # å…ˆæŒ‰æŒ‡å®šé¡ºåºæ·»åŠ å­—æ®µ
                for col in column_orders[mode]:
                    if col in item:
                        ordered_item[col] = item[col]
                # å†æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                for key, value in item.items():
                    if key not in ordered_item:
                        ordered_item[key] = value
                ordered_results.append(ordered_item)
            cleaned_results = ordered_results

        # åˆ›å»ºDataFrameï¼Œæ˜ç¡®æŒ‡å®šåˆ—é¡ºåº
        if mode in column_orders and cleaned_results:
            # è·å–å®é™…å­˜åœ¨çš„åˆ—
            all_columns = set()
            for item in cleaned_results:
                all_columns.update(item.keys())

            # æŒ‰é¢„å®šä¹‰é¡ºåºæ’åˆ—åˆ—ï¼Œç„¶åæ·»åŠ å…¶ä»–åˆ—
            ordered_columns = []
            for col in column_orders[mode]:
                if col in all_columns:
                    ordered_columns.append(col)
                    all_columns.remove(col)

            # æ·»åŠ å‰©ä½™åˆ—
            ordered_columns.extend(sorted(all_columns))

            # åˆ›å»ºDataFrameå¹¶æŒ‡å®šåˆ—é¡ºåº
            df = pd.DataFrame(cleaned_results, columns=ordered_columns)
        else:
            df = pd.DataFrame(cleaned_results)

        # ç”Ÿæˆæ–‡ä»¶å
        mode_names = {
            'sn': 'sn_papers_metadata',
            'ieee': 'ieee_papers_metadata',
            'funding': 'funding_papers_metadata',
            'ap': 'ap_papers_metadata'
        }
        filename = f"{mode_names.get(mode, 'papers_metadata')}.xlsx"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜Excelæ–‡ä»¶
        df.to_excel(file_path, index=False, engine='openpyxl')

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/progress/<task_id>', methods=['GET'])
def get_progress(task_id):
    """è·å–å¤„ç†è¿›åº¦"""
    try:
        task_info = processor.processing_tasks.get(task_id)
        if not task_info:
            return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

        return jsonify(task_info)

    except Exception as e:
        return jsonify({'error': str(e)}), 500



# =========================
# é”™è¯¯å¤„ç†
# =========================
@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'æ¥å£ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

@app.errorhandler(413)
def too_large(error):
    return jsonify({'error': 'æ–‡ä»¶è¿‡å¤§'}), 413

# =========================
# åº”ç”¨å¯åŠ¨
# =========================
if __name__ == '__main__':
    print("ğŸš€ PDFå…ƒæ•°æ®æå–ç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("ğŸ“Š æ”¯æŒçš„æå–æ¨¡å¼: SN, IEEE, èµ„åŠ©ä¿¡æ¯, AP")
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:5000")
    print("ğŸ“ ä¸Šä¼ ç›®å½•:", app.config['UPLOAD_FOLDER'])
    print("ğŸ“ ç»“æœç›®å½•:", app.config['RESULTS_FOLDER'])
    print("\nğŸ”— APIæ¥å£åˆ—è¡¨:")
    print("  GET  /                     - ä¸»é¡µç•Œé¢")
    print("  GET  /api/health           - å¥åº·æ£€æŸ¥")
    print("  POST /api/upload           - æ–‡ä»¶ä¸Šä¼ ")
    print("  GET  /api/files            - æ–‡ä»¶åˆ—è¡¨")
    print("  DEL  /api/files/<id>       - åˆ é™¤æ–‡ä»¶")
    print("  POST /api/extract/<mode>   - å•æ¨¡å¼æå–")
    print("  POST /api/extract/batch    - æ‰¹é‡æå–")
    print("  POST /api/export/excel     - å¯¼å‡ºExcel")
    print("  POST /api/export/json      - å¯¼å‡ºJSON")
    print("  POST /api/download/excel   - ç›´æ¥ä¸‹è½½Excel")
    print("  POST /process              - æµå¼å¤„ç†")
    print("\nğŸ“ æ—¥å¿—ç³»ç»Ÿå·²å¯ç”¨ï¼Œæ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨ log/ ç›®å½•")
    print("âœ¨ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…è¯·æ±‚...")

    app.run(debug=True, host='0.0.0.0', port=5000)

# -*- coding: utf-8 -*-
"""
PDFå…ƒæ•°æ®æå–ç³»ç»Ÿ - Flask APIè·¯ç”±
ä¸“æ³¨äºAPIæ¥å£å®šä¹‰ï¼Œæ•°æ®å¤„ç†é€»è¾‘å·²åˆ†ç¦»åˆ°data_processoræ¨¡å—
"""

import os
import json
import uuid
import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

from flask import Flask, request, jsonify, Response, render_template, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
from data_processor import MetadataProcessor
from concurrent_processor import get_global_processor, ConcurrentProcessor, RateLimitConfig
from config import Config
from log_manager import log_manager, log_operation, log_file_upload, log_file_processing, log_batch_processing, log_api_call

# =========================
# Flaskåº”ç”¨åˆå§‹åŒ–
# =========================
app = Flask(__name__)
app.config.from_object(Config)
CORS(app)

# åˆå§‹åŒ–ç»„ä»¶
Config.init_app(app)
processor = MetadataProcessor()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_extraction.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def get_sn_column_order(data):
    """åŠ¨æ€ç”ŸæˆSNæ¨¡å¼çš„åˆ—é¡ºåº"""
    if not data:
        return []

    # åŸºç¡€å­—æ®µï¼ˆå›ºå®šé¡ºåºï¼‰
    base_columns = [
        'Number', 'Title', 'SubTitle', 'Author count', 'All author',
        'Corresponding Author', "Corresponding author's email"
    ]

    # æ‰¾å‡ºæœ€å¤§ä½œè€…æ•°é‡
    max_authors = 0
    for item in data:
        if isinstance(item, dict):
            # ç»Ÿè®¡Author Nå­—æ®µçš„æ•°é‡
            author_count = 0
            for key in item.keys():
                if key.startswith('Author ') and key.replace('Author ', '').isdigit():
                    author_num = int(key.replace('Author ', ''))
                    author_count = max(author_count, author_num)
            max_authors = max(max_authors, author_count)

    # åŠ¨æ€ç”Ÿæˆä½œè€…å’Œå•ä½å­—æ®µ
    dynamic_columns = []
    for i in range(1, max_authors + 1):
        dynamic_columns.extend([f'Author {i}', f'Affiliation {i}'])

    return base_columns + dynamic_columns

# =========================
# APIè·¯ç”±å®šä¹‰
# =========================

@app.route('/')
def index():
    """ä¸»é¡µ - è¿”å›å‰ç«¯ç•Œé¢"""
    return render_template('PDF.html')

@app.route('/favicon.ico')
def favicon():
    """Favicon"""
    return '', 204

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

@app.route('/api/upload', methods=['POST'])
def upload_files():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    start_time = time.time()
    try:
        files = request.files.getlist('files')
        if not files or all(not file for file in files):
            log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}, time.time() - start_time, "error")
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400

        uploaded_files = []
        errors = []

        for file in files:
            if not file:
                continue

            if not file.filename:
                errors.append({'filename': 'æœªçŸ¥', 'error': 'æ–‡ä»¶åä¸ºç©º'})
                continue

            if not allowed_file(file.filename):
                errors.append({'filename': file.filename, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒPDFæ–‡ä»¶'})
                continue

            try:
                filename = secure_filename(file.filename)
                file_id = str(uuid.uuid4())
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
                file.save(file_path)

                # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                if not os.path.exists(file_path):
                    errors.append({'filename': filename, 'error': 'æ–‡ä»¶ä¿å­˜å¤±è´¥'})
                    continue

                file_size = os.path.getsize(file_path)
                if file_size == 0:
                    errors.append({'filename': filename, 'error': 'æ–‡ä»¶ä¸ºç©º'})
                    os.remove(file_path)  # åˆ é™¤ç©ºæ–‡ä»¶
                    continue

                # è®°å½•æ–‡ä»¶ä¸Šä¼ æ—¥å¿—
                log_file_upload(filename, file_size)

                uploaded_files.append({
                    'file_id': file_id,
                    'filename': filename,
                    'path': file_path,
                    'size': file_size
                })

            except Exception as e:
                errors.append({'filename': file.filename, 'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'})

        processing_time = time.time() - start_time
        
        if not uploaded_files and errors:
            log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": "æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥", "errors": errors}, processing_time, "error")
            return jsonify({
                'success': False,
                'error': 'æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥',
                'errors': errors
            }), 400

        # è®°å½•æ‰¹é‡ä¸Šä¼ æˆåŠŸæ—¥å¿—
        log_batch_processing(len(uploaded_files), "æ–‡ä»¶ä¸Šä¼ ", processing_time, len(uploaded_files), len(errors))
        
        return jsonify({
            'success': True,
            'files': uploaded_files,
            'count': len(uploaded_files),
            'errors': errors if errors else None
        })

    except Exception as e:
        processing_time = time.time() - start_time
        log_operation("æ–‡ä»¶ä¸Šä¼ ", {"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}, processing_time, "error")
        logging.error(f"æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/files', methods=['GET'])
def list_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    try:
        files = []
        upload_dir = Path(app.config['UPLOAD_FOLDER'])

        for file_path in upload_dir.glob('*.pdf'):
            stat = file_path.stat()
            # ä»æ–‡ä»¶åä¸­æå–çœŸå®æ–‡ä»¶åï¼ˆå»æ‰UUIDå‰ç¼€å’Œ.pdfæ‰©å±•åï¼‰
            full_filename = file_path.name
            if '_' in full_filename:
                # æ ¼å¼ï¼šUUID_çœŸå®æ–‡ä»¶å.pdf
                real_filename = '_'.join(full_filename.split('_')[1:])
            else:
                real_filename = full_filename

            # å»æ‰.pdfæ‰©å±•å
            if real_filename.lower().endswith('.pdf'):
                real_filename = real_filename[:-4]

            files.append({
                'filename': real_filename,  # è¿”å›å»æ‰.pdfæ‰©å±•åçš„çœŸå®æ–‡ä»¶å
                'full_filename': full_filename,  # ä¿ç•™å®Œæ•´æ–‡ä»¶åç”¨äºåˆ é™¤ç­‰æ“ä½œ
                'size': stat.st_size,
                'upload_time': datetime.fromtimestamp(stat.st_ctime).isoformat()
            })

        return jsonify({
            'files': files,
            'count': len(files)
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/files/<file_id>', methods=['DELETE'])
def delete_file(file_id):
    """åˆ é™¤æŒ‡å®šæ–‡ä»¶"""
    try:
        upload_dir = Path(app.config['UPLOAD_FOLDER'])
        file_pattern = f"{file_id}_*"

        deleted = False
        for file_path in upload_dir.glob(file_pattern):
            file_path.unlink()
            deleted = True

        if deleted:
            return jsonify({'success': True, 'message': 'æ–‡ä»¶åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/<mode>', methods=['POST'])
def extract_metadata(mode):
    """å•æ¨¡å¼å…ƒæ•°æ®æå–æ¥å£ï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰"""
    try:
        if mode not in ['sn', 'ieee', 'funding', 'ap']:
            return jsonify({'error': f'ä¸æ”¯æŒçš„æ¨¡å¼: {mode}'}), 400

        data = request.get_json()
        file_paths = data.get('file_paths', [])
        use_concurrent = data.get('use_concurrent', len(file_paths) > 5)  # è¶…è¿‡5ä¸ªæ–‡ä»¶è‡ªåŠ¨å¯ç”¨å¹¶å‘

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_files = []
        invalid_files = []
        for file_path in file_paths:
            if os.path.exists(file_path):
                valid_files.append(file_path)
            else:
                invalid_files.append({
                    'file': file_path,
                    'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
                    'status': 'failed'
                })

        if not valid_files:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„',
                'results': invalid_files
            }), 400

        # é€‰æ‹©å¤„ç†æ–¹å¼
        if use_concurrent and len(valid_files) > 1:
            # å¹¶å‘å¤„ç†
            concurrent_processor = get_global_processor()

            async def process_wrapper(file_path: str, mode: str):
                return await processor.process_file(file_path, mode)

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                results = loop.run_until_complete(
                    concurrent_processor.process_batch(valid_files, process_wrapper, mode)
                )
                # å¹¶å‘å¤„ç†å™¨å†…éƒ¨å·²ç»æŒ‰åŸå§‹ç´¢å¼•æ’åºï¼Œæ— éœ€é¢å¤–æ’åº
            finally:
                loop.close()
        else:
            # ä¸²è¡Œå¤„ç†
            results = []
            for file_path in valid_files:
                try:
                    # åœ¨Flaskçº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                    import concurrent.futures

                    def run_async_in_thread():
                        """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            return loop.run_until_complete(processor.process_file(file_path, mode))
                        finally:
                            loop.close()
                    
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥å‡½æ•°
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    
                    results.append(result)
                except Exception as e:
                    logger.error(f"ä¸²è¡Œå¤„ç†æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
                    results.append({
                        'file': file_path,
                        'filename': processor._extract_real_filename(file_path),
                        'error': str(e),
                        'status': 'failed'
                    })

        # åˆå¹¶æ— æ•ˆæ–‡ä»¶ç»“æœï¼Œå¹¶æŒ‰ç…§åŸå§‹æ–‡ä»¶è·¯å¾„é¡ºåºæ’åº
        all_results = results + invalid_files
        
        # åˆ›å»ºä¸€ä¸ªæ˜ å°„æ¥ä¿æŒåŸå§‹é¡ºåº
        file_order_map = {file_path: i for i, file_path in enumerate(file_paths)}
        
        # æŒ‰ç…§åŸå§‹æ–‡ä»¶è·¯å¾„é¡ºåºæ’åºæ‰€æœ‰ç»“æœ
        def get_original_order(result):
            file_path = result.get('file', '')
            return file_order_map.get(file_path, 999999)  # æ— æ•ˆæ–‡ä»¶æ’åœ¨æœ€å
        
        all_results.sort(key=get_original_order)
        
        successful_count = len([r for r in results if r.get('status') != 'failed' and 'error' not in r])

        return jsonify({
            'success': True,
            'mode': mode,
            'results': all_results,
            'count': len(all_results),
            'successful': successful_count,
            'failed': len(all_results) - successful_count,
            'concurrent_used': use_concurrent and len(valid_files) > 1
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/batch', methods=['POST'])
def extract_batch_optimized():
    """ä¸“é—¨çš„æ‰¹é‡å¤„ç†ç«¯ç‚¹ï¼ˆä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†ï¼‰"""
    try:
        data = request.get_json()
        file_paths = data.get('file_paths', [])
        mode = data.get('mode', 'sn')

        if mode not in ['sn', 'ieee', 'funding', 'ap']:
            return jsonify({'error': f'ä¸æ”¯æŒçš„æ¨¡å¼: {mode}'}), 400

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        if len(file_paths) > 200:
            return jsonify({'error': 'å•æ¬¡æœ€å¤šå¤„ç†200ä¸ªæ–‡ä»¶'}), 400

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_files = [f for f in file_paths if os.path.exists(f)]
        invalid_count = len(file_paths) - len(valid_files)

        if not valid_files:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„'}), 400

        # ä½¿ç”¨å¹¶å‘å¤„ç†å™¨
        concurrent_processor = get_global_processor()

        async def process_wrapper(file_path: str, mode: str):
            return await processor.process_file(file_path, mode)

        # è¿›åº¦è·Ÿè¸ª
        progress_info = {'current': 0, 'total': len(valid_files), 'message': 'å‡†å¤‡ä¸­...'}

        async def progress_callback(progress: float, message: str):
            progress_info['current'] = int(progress * len(valid_files) / 100)
            progress_info['message'] = message

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            start_time = time.time()
            results = loop.run_until_complete(
                concurrent_processor.process_batch(valid_files, process_wrapper, mode, progress_callback)
            )
            processing_time = time.time() - start_time

        finally:
            loop.close()

        # ç»Ÿè®¡ç»“æœ
        successful_count = len([r for r in results if r.get('status') != 'failed' and 'error' not in r])
        failed_count = len(results) - successful_count

        # è·å–å¤„ç†ç»Ÿè®¡
        stats = concurrent_processor.get_processing_stats()

        return jsonify({
            'success': True,
            'mode': mode,
            'results': results,
            'statistics': {
                'total_files': len(file_paths),
                'valid_files': len(valid_files),
                'invalid_files': invalid_count,
                'successful': successful_count,
                'failed': failed_count,
                'success_rate': (successful_count / len(valid_files)) * 100 if valid_files else 0,
                'processing_time': round(processing_time, 2),
                'avg_time_per_file': round(processing_time / len(valid_files), 2) if valid_files else 0
            },
            'rate_limit_stats': stats
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/processing/stats', methods=['GET'])
def get_processing_stats():
    """è·å–å½“å‰å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        concurrent_processor = get_global_processor()
        stats = concurrent_processor.get_processing_stats()

        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': time.time()
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/extract/batch_multi', methods=['POST'])
def batch_extract():
    """æ‰¹é‡å¤„ç†å¤šç§æ¨¡å¼"""
    try:
        data = request.get_json()
        file_paths = data.get('file_paths', [])
        modes = data.get('modes', ['sn'])

        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æŒ‡å®šæ–‡ä»¶è·¯å¾„'}), 400

        results = {}
        for mode in modes:
            if mode not in ['sn', 'ieee', 'funding', 'ap']:
                continue

            mode_results = []
            for file_path in file_paths:
                if not os.path.exists(file_path):
                    mode_results.append({
                        'file': file_path,
                        'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
                        'status': 'failed'
                    })
                    continue

                # å¼‚æ­¥å¤„ç†æ–‡ä»¶
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(processor.process_file(file_path, mode))
                    mode_results.append(result)
                finally:
                    loop.close()

            results[mode] = mode_results

        return jsonify({
            'success': True,
            'results': results,
            'processed_modes': list(results.keys())
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/process', methods=['POST'])
def process_files():
    """å¤„ç†PDFæ–‡ä»¶çš„ä¸»æ¥å£ï¼ˆæµå¼å“åº”ï¼‰"""
    start_time = time.time()

    def generate():
        try:
            # è·å–ä¸Šä¼ çš„æ–‡ä»¶å’Œæ¨¡å¼
            files = request.files.getlist('files')
            mode = request.form.get('mode', 'sn')

            if not files:
                log_operation("æ–‡ä»¶å¤„ç†", {"error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}, time.time() - start_time, "error")
                yield json.dumps({'type': 'error', 'message': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}) + '\n'
                return

            # è®°å½•å¼€å§‹å¤„ç†æ—¥å¿—
            log_operation("æ–‡ä»¶å¤„ç†", {"file_count": len(files), "mode": mode})

            yield json.dumps({
                'type': 'status',
                'message': f'å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼Œæ¨¡å¼: {mode}'
            }) + '\n'

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            saved_files = []
            for file in files:
                if file and allowed_file(file.filename):
                    filename = secure_filename(file.filename)
                    file_id = str(uuid.uuid4())
                    file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}_{filename}")
                    file.save(file_path)
                    saved_files.append(file_path)

            yield json.dumps({
                'type': 'info',
                'message': f'æˆåŠŸä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶'
            }) + '\n'

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            success_count = 0
            failed_count = 0

            for i, file_path in enumerate(saved_files, 1):
                file_start_time = time.time()
                # æå–çœŸå®æ–‡ä»¶åï¼ˆå»æ‰UUIDå‰ç¼€ï¼‰
                real_filename = processor._extract_real_filename(file_path)

                yield json.dumps({
                    'type': 'status',
                    'message': f'æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(saved_files)} ä¸ªæ–‡ä»¶: {real_filename}'
                }) + '\n'

                # å¼‚æ­¥å¤„ç†æ–‡ä»¶
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(processor.process_file(file_path, mode))
                    file_processing_time = time.time() - file_start_time

                    if 'error' not in result:
                        success_count += 1
                        # è®°å½•æˆåŠŸå¤„ç†æ—¥å¿—
                        log_file_processing(real_filename, mode, file_processing_time, "success")

                        yield json.dumps({
                            'type': 'data_row',
                            'data': result
                        }) + '\n'

                        yield json.dumps({
                            'type': 'success',
                            'message': f'âœ… {real_filename} å¤„ç†å®Œæˆ'
                        }) + '\n'
                    else:
                        failed_count += 1
                        # è®°å½•å¤±è´¥å¤„ç†æ—¥å¿—
                        log_file_processing(real_filename, mode, file_processing_time, "error", result["error"])

                        yield json.dumps({
                            'type': 'error',
                            'message': f'âŒ {real_filename} å¤„ç†å¤±è´¥: {result["error"]}'
                        }) + '\n'

                finally:
                    loop.close()

            # è®°å½•æ‰¹é‡å¤„ç†å®Œæˆæ—¥å¿—
            total_time = time.time() - start_time
            log_batch_processing(len(saved_files), mode, total_time, success_count, failed_count)

            yield json.dumps({
                'type': 'status',
                'message': 'æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ'
            }) + '\n'

        except Exception as e:
            processing_time = time.time() - start_time
            log_operation("æ–‡ä»¶å¤„ç†", {"error": str(e)}, processing_time, "error")
            yield json.dumps({
                'type': 'error',
                'message': f'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'
            }) + '\n'

    return Response(generate(), mimetype='text/plain')

@app.route('/api/export/excel', methods=['POST'])
def export_excel():
    """å¯¼å‡ºExcelæ ¼å¼ç»“æœ"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯å¯¼å‡º'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å‡º'}), 400

        # æ ¹æ®æ¨¡å¼å®šä¹‰å­—æ®µé¡ºåº
        column_orders = {
            'ieee': ['è®¢å•å·', 'è‹±æ–‡é¢˜ç›®', 'è‹±æ–‡å‰¯æ ‡', 'ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…é‚®ç®±'],
            'funding': ['æ–‡ä»¶å', 'è®ºæ–‡è‹±æ–‡é¢˜ç›®', 'ç¬¬ä¸€ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…å•ä½', 'é€šè®¯ä½œè€…å§“å', 'é€šè®¯ä½œè€…å•ä½',
                       'é€šè®¯ä½œè€…é‚®ç®±', 'å…³é”®è¯', 'æ‘˜è¦', 'è‡´è°¢'],
            'ap': ['æ–‡ä»¶å', 'é¢˜ç›®', 'å…³é”®è¯', 'æ‘˜è¦', 'ç¬¬ä¸€ä½œè€…å§“å', 'é€šè®¯ä½œè€…å§“å', 'å…¨éƒ¨ä½œè€…å§“å']
        }

        # è·å–å½“å‰æ¨¡å¼çš„å­—æ®µé¡ºåº
        if mode == 'sn':
            # SNæ¨¡å¼ä½¿ç”¨åŠ¨æ€åˆ—é¡ºåº
            column_order = get_sn_column_order(cleaned_results)
        elif mode in column_orders:
            column_order = column_orders[mode]
        else:
            column_order = None

        if column_order:
            # æŒ‰æŒ‡å®šé¡ºåºé‡æ–°ç»„ç»‡æ•°æ®
            ordered_results = []
            for item in cleaned_results:
                ordered_item = {}
                # å…ˆæŒ‰æŒ‡å®šé¡ºåºæ·»åŠ å­—æ®µ
                for col in column_order:
                    if col in item:
                        ordered_item[col] = item[col]
                # å†æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                for key, value in item.items():
                    if key not in ordered_item:
                        ordered_item[key] = value
                ordered_results.append(ordered_item)
            cleaned_results = ordered_results

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(cleaned_results)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{mode}_metadata_{timestamp}.xlsx"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜Excelæ–‡ä»¶
        df.to_excel(file_path, index=False, engine='openpyxl')

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/export/json', methods=['POST'])
def export_json():
    """å¯¼å‡ºJSONæ ¼å¼ç»“æœ"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯å¯¼å‡º'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å‡º'}), 400

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{mode}_metadata_{timestamp}.json"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜JSONæ–‡ä»¶
        export_data = {
            'mode': mode,
            'export_time': datetime.now().isoformat(),
            'count': len(cleaned_results),
            'results': cleaned_results
        }

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/json'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/download/excel', methods=['POST'])
def download_excel():
    """ç›´æ¥ä¸‹è½½Excelæ–‡ä»¶æ¥å£"""
    try:
        data = request.get_json()
        results = data.get('results', [])
        mode = data.get('mode', 'sn')

        if not results:
            return jsonify({'error': 'æ²¡æœ‰æ•°æ®å¯ä¸‹è½½'}), 400

        # æ¸…ç†æ•°æ®ï¼Œç§»é™¤å†…éƒ¨å¤„ç†å­—æ®µ
        cleaned_results = processor._clean_export_data(results)

        if not cleaned_results:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¸‹è½½'}), 400

        # æ ¹æ®æ¨¡å¼å®šä¹‰å­—æ®µé¡ºåº
        column_orders = {
            'ieee': ['è®¢å•å·', 'è‹±æ–‡é¢˜ç›®', 'è‹±æ–‡å‰¯æ ‡', 'ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…é‚®ç®±'],
            'funding': ['æ–‡ä»¶å', 'è®ºæ–‡è‹±æ–‡é¢˜ç›®', 'ç¬¬ä¸€ä½œè€…å§“å', 'ç¬¬ä¸€ä½œè€…å•ä½', 'é€šè®¯ä½œè€…å§“å', 'é€šè®¯ä½œè€…å•ä½',
                       'é€šè®¯ä½œè€…é‚®ç®±', 'å…³é”®è¯', 'æ‘˜è¦', 'è‡´è°¢'],
            'ap': ['æ–‡ä»¶å', 'é¢˜ç›®', 'å…³é”®è¯', 'æ‘˜è¦', 'ç¬¬ä¸€ä½œè€…å§“å', 'é€šè®¯ä½œè€…å§“å', 'å…¨éƒ¨ä½œè€…å§“å']
        }

        # æŒ‰æŒ‡å®šé¡ºåºé‡æ–°ç»„ç»‡æ•°æ®
        if mode == 'sn':
            # SNæ¨¡å¼ä½¿ç”¨åŠ¨æ€åˆ—é¡ºåº
            column_order = get_sn_column_order(cleaned_results)
        elif mode in column_orders:
            column_order = column_orders[mode]
        else:
            column_order = None

        if column_order:
            ordered_results = []
            for item in cleaned_results:
                ordered_item = {}
                # å…ˆæŒ‰æŒ‡å®šé¡ºåºæ·»åŠ å­—æ®µ
                for col in column_order:
                    if col in item:
                        ordered_item[col] = item[col]
                # å†æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                for key, value in item.items():
                    if key not in ordered_item:
                        ordered_item[key] = value
                ordered_results.append(ordered_item)
            cleaned_results = ordered_results

        # åˆ›å»ºDataFrameï¼Œæ˜ç¡®æŒ‡å®šåˆ—é¡ºåº
        if mode in column_orders and cleaned_results:
            # è·å–å®é™…å­˜åœ¨çš„åˆ—
            all_columns = set()
            for item in cleaned_results:
                all_columns.update(item.keys())

            # æŒ‰é¢„å®šä¹‰é¡ºåºæ’åˆ—åˆ—ï¼Œç„¶åæ·»åŠ å…¶ä»–åˆ—
            ordered_columns = []
            for col in column_orders[mode]:
                if col in all_columns:
                    ordered_columns.append(col)
                    all_columns.remove(col)

            # æ·»åŠ å‰©ä½™åˆ—
            ordered_columns.extend(sorted(all_columns))

            # åˆ›å»ºDataFrameå¹¶æŒ‡å®šåˆ—é¡ºåº
            df = pd.DataFrame(cleaned_results, columns=ordered_columns)
        else:
            df = pd.DataFrame(cleaned_results)

        # ç”Ÿæˆæ–‡ä»¶å
        mode_names = {
            'sn': 'sn_papers_metadata',
            'ieee': 'ieee_papers_metadata',
            'funding': 'funding_papers_metadata',
            'ap': 'ap_papers_metadata'
        }
        filename = f"{mode_names.get(mode, 'papers_metadata')}.xlsx"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)

        # ä¿å­˜Excelæ–‡ä»¶
        df.to_excel(file_path, index=False, engine='openpyxl')

        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/progress/<task_id>', methods=['GET'])
def get_progress(task_id):
    """è·å–å¤„ç†è¿›åº¦"""
    try:
        task_info = processor.processing_tasks.get(task_id)
        if not task_info:
            return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

        return jsonify(task_info)

    except Exception as e:
        return jsonify({'error': str(e)}), 500

# =========================
# é”™è¯¯å¤„ç†
# =========================
@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'æ¥å£ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

@app.errorhandler(413)
def too_large(error):
    return jsonify({'error': 'æ–‡ä»¶è¿‡å¤§'}), 413

# =========================
# åº”ç”¨å¯åŠ¨
# =========================
if __name__ == '__main__':
    print("ğŸš€ PDFå…ƒæ•°æ®æå–ç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("ğŸ“Š æ”¯æŒçš„æå–æ¨¡å¼: SN, IEEE, èµ„åŠ©ä¿¡æ¯, AP")
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:6666")
    print("ğŸ“ ä¸Šä¼ ç›®å½•:", app.config['UPLOAD_FOLDER'])
    print("ğŸ“ ç»“æœç›®å½•:", app.config['RESULTS_FOLDER'])
    print("\nğŸ”— APIæ¥å£åˆ—è¡¨:")
    print("  GET  /                     - ä¸»é¡µç•Œé¢")
    print("  GET  /api/health           - å¥åº·æ£€æŸ¥")
    print("  POST /api/upload           - æ–‡ä»¶ä¸Šä¼ ")
    print("  GET  /api/files            - æ–‡ä»¶åˆ—è¡¨")
    print("  DEL  /api/files/<id>       - åˆ é™¤æ–‡ä»¶")
    print("  POST /api/extract/<mode>   - å•æ¨¡å¼æå–")
    print("  POST /api/extract/batch    - æ‰¹é‡æå–")
    print("  POST /api/export/excel     - å¯¼å‡ºExcel")
    print("  POST /api/export/json      - å¯¼å‡ºJSON")
    print("  POST /api/download/excel   - ç›´æ¥ä¸‹è½½Excel")
    print("  POST /process              - æµå¼å¤„ç†")
    print("\nğŸ“ æ—¥å¿—ç³»ç»Ÿå·²å¯ç”¨ï¼Œæ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨ log/ ç›®å½•")
    print("âœ¨ ç³»ç»Ÿå°±ç»ªï¼Œç­‰å¾…è¯·æ±‚...")

    app.run(debug=True, host='0.0.0.0', port=5000)
